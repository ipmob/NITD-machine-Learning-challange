{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of v01 LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "7Pwq9fDJu2sG",
        "colab_type": "code",
        "outputId": "4b03ad81-f34b-477f-9604-c73a810d104a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -i https://test.pypi.org/simple/ supportlib\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://test.pypi.org/simple/\n",
            "Requirement already satisfied: supportlib in /usr/local/lib/python3.6/dist-packages (0.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BaFhanfbjPs3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0_xd0oztjDzh",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "c4eb2ffb-9622-4fbc-e6be-659dc1a35e83"
      },
      "cell_type": "code",
      "source": [
        "import supportlib.gettingdata as getdata\n",
        "getdata.kaggle()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b4a0440a-ebaf-4e49-a0a5-bd62f26a4c5f\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-b4a0440a-ebaf-4e49-a0a5-bd62f26a4c5f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "CmLSEOyhu7oS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tTl_vxGPvplJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "goHS1Q1gbtbu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "getdata.zipextract('/content/glove6b300dtxt.zip')\n",
        "#!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qq9k3g0Alk4k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d thanakomsn/glove6b300dtxt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bGotXR6-b2tH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ipmob/NITD-machine-Learning-challange"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sVa2BcBlvqCU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import re\n",
        "import keras\n",
        "\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cvx0EdUFvsqf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    \n",
        "    #Lowering case the text\n",
        "    text = text.lower()\n",
        "    \n",
        "    #Removing numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    \n",
        "    #Removing punctuations\n",
        "    import string\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    #Removing stop words\n",
        "    \n",
        "    tokens = word_tokenize(text)\n",
        "    text = ' '.join([ i for i in tokens if not i in stopwords])\n",
        "    \n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Eoq0CcnvMV_G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/NITD-machine-Learning-challange/data/train_file.csv')\n",
        "test = pd.read_csv('/content/NITD-machine-Learning-challange/data/test_file.csv')\n",
        "sample = pd.read_csv('/content/NITD-machine-Learning-challange/data/results_file.csv')\n",
        "\n",
        "data.loc[data.Subjects.isnull(), 'Subjects'] = ''\n",
        "test.loc[test.Subjects.isnull(), 'Subjects'] = ''\n",
        "\n",
        "data['combined'] = data.Subjects.str.cat(' ' + data.Title)\n",
        "test['combined'] = test.Subjects.str.cat(' ' + test.Title)\n",
        "\n",
        "dataMatType = data.MaterialType\n",
        "data.drop(labels = ['MaterialType'], axis = 1, inplace = True)\n",
        "\n",
        "data.combined = data.combined.apply(preprocess_text)\n",
        "test.combined = test.combined.apply(preprocess_text)\n",
        "'''\n",
        "data.Subjects = data.Subjects.apply(preprocess_text)\n",
        "data.Title = data.Title.apply(preprocess_text)\n",
        "test.Subjects = test.Subjects.apply(preprocess_text)\n",
        "test.Title = test.Title.apply(preprocess_text)\n",
        "'''\n",
        "flag = -1\n",
        "labelDict = {}\n",
        "\n",
        "for key in dataMatType.value_counts().keys():\n",
        "  flag += 1\n",
        "  labelDict[key] = flag\n",
        "  \n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 10                             \n",
        "\n",
        "max_words = 1000                                    \n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(data.combined)\n",
        "\n",
        "dataSequences = tokenizer.texts_to_sequences(data.combined)\n",
        "testSequences = tokenizer.texts_to_sequences(test.combined)\n",
        "\n",
        "data_word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(data_word_index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AmgGkE7lvwc0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import preprocessing\n",
        "x_train = preprocessing.sequence.pad_sequences(dataSequences, maxlen = 25)\n",
        "x_test = preprocessing.sequence.pad_sequences(testSequences, maxlen = 25)\n",
        "\n",
        "train_label = dataMatType.map(labelDict)\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "labelDataBinary = to_categorical(train_label)\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced', np.unique(dataMatType), dataMatType)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PFlbzeBoGMX9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "glove_dir = '/content'\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.300d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "embedding_dim = 300\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in data_word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_N2BPfk4Ydup",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reverseLabelDict = {}\n",
        "\n",
        "for key, value in labelDict.items():\n",
        "  reverseLabelDict[value] = key\n",
        "def predict(model, x_test):\n",
        "  predictions = model.predict(x_test)\n",
        "  predictions = pd.Series([np.argmax(i) for i in predictions], index = test.index)\n",
        "  predictions = predictions.map(reverseLabelDict)\n",
        "  return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "p8lV4j7qvUNd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Embedding, SimpleRNN, Conv1D, MaxPooling1D\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Dense\n",
        "\n",
        "model1 = Sequential()\n",
        "model1.add(Embedding(1000, 1000, input_length=25))\n",
        "\n",
        "'''model.add(Conv1D(64, 5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=4))\n",
        "'''\n",
        "\n",
        "\n",
        "model1.add(Conv1D(128, 3,  activation='relu'))\n",
        "model1.add(Dropout(0.3))\n",
        "\n",
        "model1.add(LSTM(400, recurrent_dropout = 0.6, return_sequences = True))\n",
        "model1.add(Dropout(0.6))\n",
        "\n",
        "model1.add(LSTM(400, recurrent_dropout = 0.2))\n",
        "model1.add(Dropout(0.2))\n",
        "\n",
        "model1.add(Dense(8, activation='softmax'))\n",
        "\n",
        "model1.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model1.fit(x_train, labelDataBinary,\n",
        "                    epochs=16,\n",
        "                    validation_split=0.2,\n",
        "                    class_weight = class_weights)\n",
        "\n",
        "model1.save('/content/m1.h5')\n",
        "#0.86623 \n",
        "\n",
        "predic1 = pd.Series(predict(model1, x_test))\n",
        "submission = pd.concat([test.ID.astype(np.int), predic1], axis = 1)\n",
        "submission.columns = ['ID' , 'MaterialType']\n",
        "submission.to_csv('/content/submition_model1.csv', index = False, header = True)  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yfkfElIialcz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-rmb-dVhiUVV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tR9caCGEhEKS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import CuDNNLSTM,CuDNNGRU\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Embedding, SimpleRNN, Conv1D, MaxPooling1D\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Dense\n",
        "from keras import optimizers,callbacks\n",
        "import keras\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(1000, 1000 ,input_length=25))\n",
        "\n",
        "\n",
        "\n",
        "model2.add(Conv1D(256, 5, activation='elu'))\n",
        "model2.add(Dropout(0.6))\n",
        "model2.add(Conv1D(512, 5, activation='elu'))\n",
        "model2.add(Dropout(0.6))\n",
        "model2.add(Conv1D(1024, 5, activation='elu'))\n",
        "model2.add(Dropout(0.6))\n",
        "\n",
        "model2.add(Flatten())\n",
        "\n",
        "model2.add(Dense(400, activation='elu'))\n",
        "model2.add(Dense(100, activation='elu'))\n",
        "model2.add(Dense(8, activation='softmax'))\n",
        "\n",
        "#adam = optimizers.adam(lr = 0.0005)\n",
        "adam = optimizers.adam(lr = 0.0026)\n",
        "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\n",
        "model2.compile(optimizer=adam,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model2.fit(x_train, labelDataBinary,\n",
        "                    epochs=16,\n",
        "                    validation_split=0.2,\n",
        "                    class_weight = class_weights,\n",
        "                   callbacks=[reduce_lr])\n",
        "\n",
        "model2.save('/content/m2.h5')\n",
        "#0.85621\n",
        "predic2 = pd.Series(predict(model2, x_test))\n",
        "submission = pd.concat([test.ID.astype(np.int), predic2], axis = 1)\n",
        "submission.columns = ['ID' , 'MaterialType']\n",
        "submission.to_csv('/content/submition_model2.csv', index = False, header = True)  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K-vJg1_cvU9W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import CuDNNLSTM,CuDNNGRU\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Embedding, SimpleRNN, Conv1D, MaxPooling1D\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Dense\n",
        "from keras import optimizers,callbacks\n",
        "import keras\n",
        "model3 = Sequential()\n",
        "model3.add(Embedding(1000, 1000 ,input_length=25))\n",
        "\n",
        "\n",
        "model3.add(CuDNNGRU(600, return_sequences = True))\n",
        "model3.add(Dropout(0.6))\n",
        "\n",
        "model3.add(CuDNNGRU(600, return_sequences = True))\n",
        "model3.add(Dropout(0.6))\n",
        "\n",
        "model3.add(CuDNNGRU(800))\n",
        "model3.add(Dropout(0.7))\n",
        "\n",
        "model3.add(Dense(200, activation='elu'))\n",
        "model3.add(Dense(50, activation='elu'))\n",
        "model3.add(Dense(8, activation='softmax'))\n",
        "\n",
        "adam = optimizers.adam(lr = 0.001)\n",
        "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\n",
        "model3.compile(optimizer=adam,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model3.fit(x_train, labelDataBinary,\n",
        "                    epochs=16,\n",
        "                    validation_split=0.02,\n",
        "                    class_weight = class_weights,\n",
        "                   callbacks=[reduce_lr])\n",
        "\n",
        "model3.save('/content/m3.h5')\n",
        "#0.87688 \n",
        "predic3 = pd.Series(predict(model3, x_test))\n",
        "submission = pd.concat([test.ID.astype(np.int), predic3], axis = 1)\n",
        "submission.columns = ['ID' , 'MaterialType']\n",
        "submission.to_csv('/content/submition_model3.csv', index = False, header = True)  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4CUA9qwchNgt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import CuDNNLSTM,CuDNNGRU\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Embedding, SimpleRNN, Conv1D, MaxPooling1D\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Dense\n",
        "from keras import optimizers,callbacks\n",
        "import keras\n",
        "model4 = Sequential()\n",
        "model4.add(Embedding(1000, 1000 ,input_length=25))\n",
        "\n",
        "\n",
        "model4.add(CuDNNGRU(600, return_sequences = True))\n",
        "model4.add(Dropout(0.7))\n",
        "\n",
        "model4.add(CuDNNGRU(600))\n",
        "model4.add(Dropout(0.5))\n",
        "\n",
        "model4.add(Dense(100, activation='elu'))\n",
        "model4.add(Dense(50, activation='elu'))\n",
        "model4.add(Dense(8, activation='softmax'))\n",
        "\n",
        "adam = optimizers.adam(lr = 0.001)\n",
        "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\n",
        "model4.compile(optimizer=adam,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model4.fit(x_train, labelDataBinary,\n",
        "                    epochs=16,\n",
        "                    validation_split=0.2,\n",
        "                    class_weight = class_weights,\n",
        "                   callbacks=[reduce_lr])\n",
        "\n",
        "model4.save('/content/m4.h5')\n",
        "#0.87181 \n",
        "predic4 = pd.Series(predict(model4, x_test))\n",
        "submission = pd.concat([test.ID.astype(np.int), predic4], axis = 1)\n",
        "submission.columns = ['ID' , 'MaterialType']\n",
        "submission.to_csv('/content/submition_model4.csv', index = False, header = True)  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7OmSE-mdtAW-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a8EGozWZ3UdC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Using Embedding Layer**"
      ]
    },
    {
      "metadata": {
        "id": "CNdQiMIGwjQD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Embedding, SimpleRNN, Conv1D, MaxPooling1D\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Dense\n",
        "\n",
        "model5 = Sequential()\n",
        "model5.add(Embedding(1000, 300, input_length=25, weights=[embedding_matrix], trainable=False))\n",
        "\n",
        "model5.add(Conv1D(64, 5, activation='relu'))\n",
        "model5.add(MaxPooling1D(pool_size=4))\n",
        "\n",
        "\n",
        "model5.add(LSTM(1000, recurrent_dropout = 0.3))\n",
        "model5.add(Dropout(0.35))\n",
        "model5.add(Dense(8, activation='softmax'))\n",
        "\n",
        "model5.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model5.fit(x_train, labelDataBinary,\n",
        "                    epochs=16,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2,\n",
        "                    class_weight = class_weights)\n",
        "\n",
        "\n",
        "model5.save('/content/m5.h5')\n",
        "predic5 = pd.Series(predict(model5, x_test))\n",
        "submission = pd.concat([test.ID.astype(np.int), predic5], axis = 1)\n",
        "submission.columns = ['ID' , 'MaterialType']\n",
        "submission.to_csv('/content/submition_model5.csv', index = False, header = True)  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vrryo7yGt_Mc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import CuDNNLSTM,CuDNNGRU\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Embedding, SimpleRNN, Conv1D, MaxPooling1D\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Dense\n",
        "from keras import optimizers,callbacks\n",
        "import keras\n",
        "model6 = Sequential()\n",
        "model6.add(Embedding(1000, 1000 ,input_length=25))\n",
        "\n",
        "\n",
        "model6.add(CuDNNGRU(600, return_sequences = True))\n",
        "#model.add(Dropout(0.6))\n",
        "model6.add(Dropout(0.7))\n",
        "\n",
        "model6.add(CuDNNGRU(500))\n",
        "model6.add(Dropout(0.6))\n",
        "\n",
        "model6.add(Dense(100, activation='elu'))\n",
        "model6.add(Dense(50, activation='elu'))\n",
        "model6.add(Dense(8, activation='softmax'))\n",
        "\n",
        "adam = optimizers.adam(lr = 0.001)\n",
        "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\n",
        "model6.compile(optimizer=adam,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model6.fit(x_train, labelDataBinary,\n",
        "                    epochs=16,\n",
        "                    validation_split=0.2,\n",
        "                    class_weight = class_weights,\n",
        "                   callbacks=[reduce_lr])\n",
        "\n",
        "model6.save('/content/model6.1.h5')\n",
        "#v1_0.87181 \n",
        "#0.87181 \n",
        "#0.87695\n",
        "predic6 = pd.Series(predict(model6, x_test))\n",
        "submission = pd.concat([test.ID.astype(np.int), predic6], axis = 1)\n",
        "submission.columns = ['ID' , 'MaterialType']\n",
        "submission.to_csv('/content/submition_model6.csv', index = False, header = True)  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZReVSR3gy5kI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "reverseLabelDict = {}\n",
        "\n",
        "for key, value in labelDict.items():\n",
        "  reverseLabelDict[value] = key\n",
        "  \n",
        "predictions = model.predict(x_test)\n",
        "predictions = pd.Series([np.argmax(i) for i in predictions], index = test.index)\n",
        "predictions = predictions.map(reverseLabelDict)\n",
        "\n",
        "submission = pd.concat([test.ID.astype(np.int), predictions], axis = 1)\n",
        "submission.columns = ['ID' , 'MaterialType']\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MJCiVVuTQ-x6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " # submission.to_csv('/content/submition_3.1_05_100_50_8.csv', index = False, header = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w4azj14Q4A2R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\"reverseLabelDict = {}\n",
        "\n",
        "for key, value in labelDict.items():\n",
        "  reverseLabelDict[value] = key\n",
        "def predict(model, x_test):\n",
        "  predictions = model.predict(x_test)\n",
        "  predictions = pd.Series([np.argmax(i) for i in predictions], index = test.index)\n",
        "  predictions = predictions.map(reverseLabelDict)\n",
        "  submission = pd.concat([test.ID.astype(np.int), predictions], axis = 1)\n",
        "  submission.columns = ['ID' , 'MaterialType']\n",
        "  return predictions\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ie1SXk352-SI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_model = 6\n",
        "predic1 = pd.Series(predict(model1, x_test))\n",
        "predic2 = pd.Series(predict(model2, x_test))\n",
        "predic3 = pd.Series(predict(model3, x_test))\n",
        "predic4 = pd.Series(predict(model4, x_test))\n",
        "predic5 = pd.Series(predict(model5, x_test))\n",
        "predic6 = pd.Series(predict(model6, x_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4RjMF0-A7FJZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"total = pd.DataFrame(pd.concat([predic1, predic2, predic3, predic4, predic5,predic6], axis = 1))\n",
        "submissions = total.mode(axis = 1)[0]\n",
        "submission = pd.concat([test.ID.astype(np.int), predictions], axis = 1)\n",
        "submission.columns = ['ID' , 'MaterialType']\n",
        "submission.to_csv('sub-agg.csv', index = False, header = True)\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LrPJl9KF8O40",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "total = pd.DataFrame(pd.concat([predic1, predic2, predic3, predic4, predic5,predic6], axis = 1))\n",
        "submissions = total.mode(axis = 1)[0]\n",
        "#submissions = pd.concat([test.ID.astype(np.int), predictions], axis = 1)\n",
        "submissions.columns = ['ID' , 'MaterialType']\n",
        "submissions.to_csv('/content/sub-final.csv', index = False, header = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y-9IE2z4V912",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}